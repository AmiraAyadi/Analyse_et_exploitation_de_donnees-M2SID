#!/usr/bin/python
# coding: utf-8
import random
import numpy as np
import numpy.linalg as la
import matplotlib.pyplot as plt 
import numpy as np
import matplotlib as mpl
import itertools
import csv
import pylab as pl
import time
import os
import scipy
import sklearn.mixture as skm
from math import *
from numpy import append, zeros
from scipy.io import wavfile
from sklearn.mixture import GMM
from scipy import linalg

def hamming2(n):
    """ Define Hamming Window and use it to do short 1me fourier transform """
    return 0.54 - 0.46* np.cos(2*pi/n * np.arange(n))

def mel(nFilters, FFTLen, sampRate): 
    halfFFTLen = int(floor(FFTLen/2)) 
    M = zeros((nFilters, halfFFTLen)) 
    lowFreq = 20 # Hz
    highFreq = 8000 # Hz
    melLowFreq = 1125*np.log(1+lowFreq/700.0)
    melHighFreq = 1125*np.log(1+highFreq/700.0)
    melStep = int(floor((melHighFreq - melLowFreq)/nFilters)) 
    melLow2High = np.arange(melLowFreq, melHighFreq, melStep) 
    #melLow2High = 1125*np.log(1+np.arange(lowFreq, highFreq)/700.0)
    HzLow2High = 700*(np.exp(melLow2High/1125)-1) 
    HzLow2HighNorm = np.floor(FFTLen*HzLow2High/sampRate)

    # form the triangular filters 
    for filt in range(nFilters):
        xStart1 = HzLow2HighNorm[filt] 
        xStop1 = HzLow2HighNorm[filt+1] 
        yStep1 = 1/(xStop1-xStart1) 
        M[filt, int(xStart1)] = 0.0;
        for x in np.arange(xStart1+1, xStop1): 
            M[filt, int(x)] = M[filt, int(x)-1] + yStep1
    for filt in range(nFilters-1):
        xStart2 = HzLow2HighNorm[filt+1]
        xStop2 = HzLow2HighNorm[filt+2] 
        yStep2 = -1.0/(xStop2-xStart2)
        M[filt, int(xStart2)] = 1.0;
        for x in np.arange(xStart2+1, xStop2):
            M[filt, int(x)] = M[filt, int(x)-1] + yStep2
    return melLow2High, HzLow2High, HzLow2HighNorm, M

def dctmtx(n):
    #DCT-II matrix
    x,y = np.meshgrid(range(n), range(n))
    D = np.sqrt(2.0/n) * np.cos(pi * (2*x+1) * y / (2*n)) 
    D[0] /= np.sqrt(2)
    return D

def mfcc(filename):
    ''' MFCC feature extractor
        
        From: Yangshun Tay https://gist.github.com/yangshun/9183815'''
    sampleRate, signal = wavfile.read(filename)
    FrameDuration = 0.020 # 20 milliseconds window
    FrameLen = int(FrameDuration * sampleRate) # number of points in one window
    FrameShift = int(FrameLen / 2) # overlapping window
    FFTLen = 2048
    win = hamming2(FrameLen)
    
    # spectrogram
    lenSig = len(signal)
    nframes = int((lenSig-FrameLen)/FrameShift)
    nFilters = 40
    mfccCoefs = 13
    preEmphFactor = 0.95 #array to hold spectrogram
    powSpec2D = np.zeros((FFTLen,nframes))
    mfcc2D = np.zeros((mfccCoefs,nframes))
    mfcc2DSpec = np.zeros((nFilters,nframes))
    mfcc2DPow = np.zeros((nFilters,nframes))
    melLow2High, HzLow2High, HzLow2HighNorm, M = mel(nFilters, FFTLen, sampleRate)
    D = dctmtx(nFilters)[1:mfccCoefs+1]
    invD = la.inv(dctmtx(nFilters))[:,1:mfccCoefs+1]
    minPowSpec = 1e-50

    for fr in range(0, nframes):
        start = fr*FrameShift
        currentFrame = signal[start:start+FrameLen]
        #--- pre-emphasis filtering
        #currentFrame[1:] -= currentFrame[:-1] * preEmphFactor
    
        currentFrame1 = currentFrame * win
    
        #--- pre-emphasis filtering
        currentFrame1[1:] -= currentFrame1[:-1] * preEmphFactor
    
        #--- fourier transform using numpy
        fftCurrentFrame = np.fft.fft(currentFrame1, FFTLen)
        fftCurrentFrame[abs(fftCurrentFrame) < minPowSpec] = minPowSpec

        shiftedFFT = np.fft.fftshift(fftCurrentFrame)
        powSpec = 20*np.log(np.abs(shiftedFFT))

        #--- store current frame's power spectrum
        powSpec2D[:,fr] = powSpec
        #mfcc2D[:, fr] = np.log(np.dot(M, np.abs(shiftedFFT)**2))
        mfcc2DPow[:, fr] = np.log(np.dot(M, np.abs(fftCurrentFrame[0:int(FFTLen/2)])**2))
        mfcc2D[:, fr] = np.dot(D, mfcc2DPow[:,fr])

    # normalize the MFCC coefficients
    meanFeat = np.mean(mfcc2D, axis=0)
    sigmaFeat = np.std(mfcc2D, axis=0)
    mfcc2D = (mfcc2D - meanFeat)/sigmaFeat

    for fr in range(0, nframes-1):
        mfcc2DSpec[:, fr] = np.dot(invD, mfcc2D[:,fr].T)

    return mfcc2D.T

def gmm_mllr_diag_cov(X, gmm, niter=10):
  """ 
     GMM adaptation (only means) using MLLR for GMM with diagonal covariance matrix.

     Usage: gmm_mllr_diag_cov(X, gmm, niter)
     
     References: Leggetter & Woodland'1995
     
     From: Kevin Hu https://github.com/mrhuke/GMM-adaptation
  """

  # remove illed gaussians
  logprob=gmm.score_samples(X)
  pcompx=gmm.predict_proba(X)
  ###logprob,pcompx = gmm.eval(X)
  psum = np.sum(pcompx, axis=0)
  ill_g = (psum == 0);
  if any(ill_g):
    valid = psum > 0
    gmm.means_ = gmm.means_[valid,:]
    gmm.weights_ = gmm.weights_[valid]
    gmm.weights_ = gmm.weights_/sum(gmm.weights_)
    gmm.covariances_ = gmm.covariances_[valid]
    ###logprob,pcompx = gmm.eval(X)
    logprob=gmm.score_samples(X)
    pcompx=gmm.predict_proba(X)

  # calculate G and Z 
  C = len(gmm.weights_)
  T,dim = X.shape
  W = np.empty([dim,dim+1])
  G = np.zeros([dim,dim+1,dim+1])
  # 1. first calculate D[0,...,C) and Z
  D = np.zeros([C,dim+1,dim+1])
  V = np.zeros([C,dim,dim])
  Z = np.zeros([dim,dim+1])
  for c in range(0,C):
    mu = gmm.means_[c]
    sigma = np.diag(gmm.covariances_[c])
    sigma_inv = np.linalg.inv(sigma)
    p = pcompx[:,c]

    xi = np.empty_like(mu)
    xi[:] = mu
    xi = np.insert(xi,0,1)
    xi = np.reshape(xi, [len(xi),1])
    D[c] = xi.dot(xi.T)
    V[c] = np.sum(p)*sigma_inv
    for t in range(0,T):
      xt = np.reshape(X[t],[len(X[t]),1])
      Z += p[t]*sigma_inv.dot(xt).dot(xi.T)
    
  # 2. now calculate G
  for i in range(0,dim): 
    for c in range(0,C): # tie all Gaussians
      G[i] += V[c,i,i]*D[c]
    try: 
      G_i_inv = np.linalg.inv(G[i])
    except:
      print('G is nearly singular and pseudo-inverse is used.')
      G_i_inv = np.linalg.pinv(G[i])
    z_i = np.reshape(Z[i],[dim+1,1])
    W[i] = G_i_inv.dot(z_i)[:,0]
  
  # transform means
  for c in range(0,C):
    xi = np.insert(gmm.means_[c],0,1)
    xi = np.reshape(xi,[len(xi),1])
    gmm.means_[c] = W.dot(xi)[:,0]
      
  # remove non positive definite matrices
  ###logprob,pcompx = gmm.eval(X)
  logprob=gmm.score_samples(X)
  pcompx=gmm.predict_proba(X)
  psum = np.sum(pcompx, axis=0)
  ill = (psum == 0);
  if np.any(ill):
    valid = (ill == 0)
    gmm.means_ = gmm.means_[valid]
    gmm.weights_ = gmm.weights_[valid]
    gmm.weights_ = gmm.weights_/sum(gmm.weights_)
    gmm.covariances_ = gmm.covariances_[valid]
    K = gmm.means_.shape[0]
  return gmm

def plot_gmm(gmm,X):
    color_iter = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'])
    for i, (clf, title) in enumerate([(gmm, 'GMM')]):
        print(clf,title,i)
        splot = pl.subplot(1, 1, 1 + i)
        Y_ = clf.predict(X)
        for i, (mean, covar, color) in enumerate(zip(
                clf.means_, clf.covariances_, color_iter)):
            #v, w = linalg.eigh(np.diag(covar))
            v, w = linalg.eigh(covar)
            v = 2. * np.sqrt(2.) * np.sqrt(v) ###
            u = w[0] / linalg.norm(w[0])
        # as the DP will not use every component it has access to
        # unless it needs it, we shouldn't plot the redundant
        # components.
        if not np.any(Y_ == i):
            continue
        pl.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)

        #Plot an ellipse to show the Gaussian component
        angle = np.arctan(u[1] / u[0])
        angle = 180 * angle / np.pi  # convert to degrees
        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)
        ell.set_clip_box(splot.bbox) ###
        ell.set_alpha(0.5)
        splot.add_artist(ell)

    #plt.xlim(-9., 5.)
    #plt.ylim(-3., 6.)
    pl.xlim(-10, 10)
    pl.ylim(-10, 10)
    pl.xticks(())
    pl.yticks(())
    pl.title(title)
    pl.show()

def plot_gmm2(gmm,X):
    color_iter = itertools.cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'])
    for i, (clf, title) in enumerate([(gmm, 'GMM')]):
        print(clf,title,i)
        splot = pl.subplot(1, 1, 1 + i)
        Y_ = clf.predict(X)
        for i, (mean, covar, color) in enumerate(zip(
                clf.means_, clf.covariances_, color_iter)):
            v, w = linalg.eigh(np.diag(covar))
            #v, w = linalg.eigh(covar)
            v = 2. * np.sqrt(2.) * np.sqrt(v) ###
            u = w[0] / linalg.norm(w[0])
        # as the DP will not use every component it has access to
        # unless it needs it, we shouldn't plot the redundant
        # components.
        if not np.any(Y_ == i):
            continue
        pl.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)

        #Plot an ellipse to show the Gaussian component
        angle = np.arctan(u[1] / u[0])
        angle = 180 * angle / np.pi  # convert to degrees
        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)
        ell.set_clip_box(splot.bbox) ###
        ell.set_alpha(0.5)
        splot.add_artist(ell)

    #plt.xlim(-9., 5.)
    #plt.ylim(-3., 6.)
    pl.xlim(-10, 10)
    pl.ylim(-10, 10)
    pl.xticks(())
    pl.yticks(())
    pl.title(title)
    pl.show()


def gmm_map_qb(X, gmm, rho=.1, epsilon=1, niter=10):
  """ 
     GMM adaptation using Quasi-Bayes MAP method.

     Usage: gmm_map_qb(X, gmm, rho, epsilon, niter)

     //Ref. 1) (1994 Gauvain, Lee) Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains
     //Ref. 2) (1997 Huo, Lee) On-line adaptive learning of the continuous density hidden markov model based on approximate recursive bayes estimate
     //Ref. 3) (2010 Kim, Loizou) Improving Speech Intelligibility in Noise Using Environment-Optimized Algorithms
      
      From: Kevin Hu https://github.com/mrhuke/GMM-adaptation
"""

  # init
  ###logprob,pcompx = gmm.eval(X)
  logprob=gmm.score_samples(X)
  pcompx=gmm.predict_proba(X)
  psum = np.sum(pcompx, axis=0) #(18)
  # remove illed gaussians
  ill_g = (psum == 0);
  if any(ill_g):
    valid = psum > 0
    gmm.means_ = gmm.means_[valid,:]
    gmm.weights_ = gmm.weights_[valid]
    gmm.weights_ = gmm.weights_/sum(gmm.weights_)
    gmm.covariances_ = gmm.covariances_[valid]

  ###logprob,pcompx = gmm.eval(X)
  logprob=gmm.score_samples(X)
  pcompx=gmm.predict_proba(X)

  psum = np.sum(pcompx, axis=0) #(18)
  K,nDim = gmm.means_.shape
  tau = psum*epsilon #(22)
  tau_update = tau
  nu = 1 + tau #(23)
  nu_update = nu
  alpha = nDim + tau #(24)
  alpha_update = alpha
  mu = np.empty([K,nDim])
  mu_update = np.empty([K,nDim])
  yu = np.empty([K,nDim,nDim])
  yu_update = np.empty([K,nDim,nDim])
  for k in range(0,K):
    mu[k] = gmm.means_[k] #(25)
    yu[k] = tau[k] * gmm.covariances_[k] #(26)

  # EM iterations 
  s = np.empty([K,nDim,nDim])
  N = X.shape[0]
  for iter in range(0, niter):
    print('iter=',iter)
    print(np.sum(gmm.weights_))
    #plot_gmm(gmm,X)
    # E-step: posterior probabilities
    ###logprob, pcompx = gmm.eval(X)
    logprob=gmm.score_samples(X)
    pcompx=gmm.predict_proba(X)
    
    # remove illed gaussians
    psum = np.sum(pcompx, axis=0)  # (18)
    #print psum
    #raw_input()
    ill_g = (psum == 0);
    #print ill_g
    if any(ill_g):
      valid = psum > 0
      gmm.means_ = gmm.means_[valid,:]
      gmm.weights_ = gmm.weights_[valid]
      gmm.weights_ = gmm.weights_/sum(gmm.weights_)
      gmm.covariances_ = gmm.covariances_[valid]
      mu = mu[valid]
      mu_update = mu_update[valid]
      yu = yu[valid]
      yu_update = yu_update[valid]
      tau = tau[valid]
      tau_update = tau_update[valid]
      alpha = alpha[valid]
      alpha_update = alpha_update[valid]
      nu = nu[valid]
      nu_update = nu_update[valid]
      K = gmm.means_.shape[0]
      continue

    # M-step, eqs. from KimLoizou'10
    # Hyper-parameters
    psum = np.sum(pcompx, axis=0)  # (18)
    #print np.sum(pcompx,axis=1)
    #print 'psum',psum
    #print gmm.weights_
    #raw_input()
    x_expected = np.dot(pcompx.T, X)/np.tile(psum[np.newaxis].T,(1,nDim)) #(19)
    #print 'x_expected',x_expected
    for k in range(0,K):
      #raw_input()
      # (20)
      s[k] = np.dot((X-np.tile(x_expected[k],(N,1))).T, (X-np.tile(x_expected[k],(N,1)))*np.tile(pcompx[:,k][np.newaxis].T,(1,nDim)))
      # (15)
      #print 'yu[k]',yu[k] 
      #print 'x_expected - mu', (x_expected-mu).T  
      #yu_update[k] = rho*yu[k] + s[k] + rho*tau[k]*psum[k]/(rho*tau[k]+psum[k])*np.dot((x_expected-mu).T, x_expected-mu)   
      yu[k] = rho*yu[k] + s[k] + rho*tau[k]*psum[k]/(rho*tau[k]+psum[k])*np.dot((x_expected[k,:]-mu[k,:]).T, x_expected[k,:]-mu[k,:])   
      #print 'yu[k],after',yu[k]  
 
    # (21)
    beta = psum/(rho*tau+psum)
    # mu, eq (14)
    #print beta.shape
    #print mu.shape
    #print x_expected.shape
    #mu_update = np.tile(beta[np.newaxis].T,(1,nDim))*x_expected + np.tile((1-beta)[np.newaxis].T,(1,nDim))*mu
    mu = np.tile(beta[np.newaxis].T,(1,nDim))*x_expected + np.tile((1-beta)[np.newaxis].T,(1,nDim))*mu
    # tau, eq (11)
    #tau_update = rho*tau + psum
    tau = rho*tau + psum
    # alpha, eq (12)
    #alpha_update = rho*(alpha-nDim) + nDim + psum    
    alpha = rho*(alpha-nDim) + nDim + psum    
    #print 'alpha',alpha
    # nu, eq (13)
    #nu_update = rho*(nu-1) + 1 + psum
    nu = rho*(nu-1) + 1 + psum
  
    # GMM parameters
    # weight, (27)
    #gmm.weights_ = (nu_update-1)/np.sum(nu_update-1)
    gmm.weights_ = (nu-1)/np.sum(nu-1)
    ill = (gmm.weights_ == 0)
    print(np.sum(gmm.weights_))
    # mean, (28)
    #gmm.means_ = mu_update   
    gmm.means_ = mu   
    # sigma, (29)
    for k in range(0,K):
      #if alpha_update[k] != nDim:
      if alpha[k] != nDim:
        #gmm.covars_[k] = yu_update[k]/(alpha_update[k]-nDim)
        gmm.covariances_[k] = yu[k]/(alpha[k]-nDim)
      else:
        #gmm.covars_[k] = yu_update[k]/tau_update[k]
        gmm.covariances_[k] = yu[k]/tau[k]
      try:
        np.linalg.cholesky(gmm.covariances_[k])
      except:
        ill[k] = 1
        print('cov_%d not positive definite' % k)
    
    # remove non positive definite matrices
    if np.any(ill):
      valid = (ill == 0)
      gmm.means_ = gmm.means_[valid]
      gmm.weights_ = gmm.weights_[valid]
      gmm.weights_ = gmm.weights_/sum(gmm.weights_)
      gmm.covariances_ = gmm.covariances_[valid]
      mu = mu[valid]
      mu_update = mu_update[valid]
      yu = yu[valid]
      yu_update = yu_update[valid]
      tau = tau[valid]
      tau_update = tau_update[valid]
      alpha = alpha[valid]
      alpha_update = alpha_update[valid]
      nu = nu[valid]
      nu_update = nu_update[valid]
      K = gmm.means_.shape[0]
  return gmm

def vad_filter(matriceMFCC):
    ''' Filtrage de la matrice des MFCC pour ne conserver que les lignes qui correspondent a de la parole
        --- A FAIRE ---
        Algorithme :
        1. calcul du C0 moyen de la totalite des lignes
        3. Choix d'un seuil en fonction de C0moyen (par exemple : 0.7*C0moyen)
        4. Suppression des lignes dont le C0 est inferieure a ce seuil
        5. Suppression de la colonne C0 '''
    # --- A FAIRE ----
    c0moyen = matriceMFCC[:,12].mean()
    seuil = 0.7*c0moyen
    mask = matriceMFCC[:,12]<seuil
    matriceMFCCfiltre = matriceMFCC[mask]
    matriceMFCCfiltre = matriceMFCCfiltre[:,:-1]
    return matriceMFCCfiltre


# --- PROGRAMME PRINCIPAL ---
# ---     A completer !   ---

X_train=np.empty([1,12]) # adapter a 12 si la ligne C0 est supprimee
List = open("./LISTS/ubm-train.lst")
for filename in List:
    # pour chaque fichier de l'apprentissage de l'UBM
    # 1. calcul des MFCC
    matriceMFCC=mfcc("./"+filename.rstrip())
#    plt.imshow(matriceMFCC)
#    plt.imshow(matriceMFCC[1:30,:].T)
#    plt.show()
#    pl.hist(matriceMFCC[:,12:13]) 
#    pl.show()
    #plt.imshow(matriceMFCC[1:30,1:2].T)
    #plt.show()
    # 2. filtrage VAD
    # ...fonction vad_filter a ecrire (cf. plus haut)...
    matriceMFCCfiltre=vad_filter(matriceMFCC)
    #print(np.shape(matriceMFCCfiltre))
    # concatenation dans une matrice qui contient tous les MFCC filtres
    X_train = np.vstack((X_train,matriceMFCCfiltre))

print(np.shape(X_train))

#Question 2
#X = X_train[0:10000]
from sklearn.mixture import GaussianMixture

for cov_type in ['spherical', 'diag', 'tied', 'full']:
    for init_para in ['kmeans','random']:
        for i in range(5,10):
            clf = GaussianMixture(n_components=i, covariance_type=cov_type, init_params=init_para)
            gmm = clf.fit(X_train)
            try :
                plot_gmm(gmm,X_train)
            except:
                try:
                    plot_gmm2(gmm,X_train)
                except:
                    print("nope")

#afficher la premiere dim 
pl.hist(X_train[:,0],200, range=(-5,5))
pl.show()
#afficher la seconde dim 
pl.hist(X_train[:,1],200, range=(-5,5))
pl.show()




